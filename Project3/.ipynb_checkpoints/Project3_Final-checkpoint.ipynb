{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tbfz3zSXBt3H"
   },
   "source": [
    "## <p style=\"text-align: center;\">MIS 284N - Big Data and Distributed Programming</p>\n",
    "## <p style=\"text-align: center;\">Project 3 - Machine Learning using Tensorflow and Google Colab</p>\n",
    "## <p style=\"text-align: center;\">By: Gina Huh(jh62434), David Kinman(dk27285), Whitt Hyde(wjh695)</p>\n",
    "## <p style=\"text-align: center;\">Total points: 100</p>\n",
    "## <p style=\"text-align: center;\">Due: Sunday, October 20th submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.\n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)\n",
    "\n",
    "This project is about giving exposure about Tensorflow, its usage, Cloud services and help us in understanding the time taken to run computation on CPU and GPU. \n",
    "\n",
    "In this Project, we will work with CIFAR10 image dataset. \n",
    "The starter code to download the dataste using keras is given below. \n",
    "You should run this project on Google Colab. You would be using CPU, GPU.\n",
    "Use tensorflow version 2.0. \n",
    "\n",
    "# In every line of code, please write a comment to briefly explain what that line is doing.\n",
    "Your grades will be based on your understanding of the code you write! \n",
    "\n",
    "Note: The code you write should be your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RnHTzAuvxxQT"
   },
   "source": [
    "# Task 1\n",
    "Convert the features in a form that can be given as input to tensorflow library/functions\n",
    "\n",
    "In this task you will perform data augmentation. That is, pre-process the data to make the model more robust. Most common data augmentation techniques are rotation, flips and histogram equalization. \n",
    "You can choose an augmentation technique of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\david\\anaconda3\\setup_dk\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\david\\appdata\\roaming\\python\\python37\\site-packages (from keras) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\david\\appdata\\roaming\\python\\python37\\site-packages (from keras) (1.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\david\\appdata\\roaming\\python\\python37\\site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\david\\anaconda3\\setup_dk\\lib\\site-packages (from keras) (5.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\david\\anaconda3\\setup_dk\\lib\\site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: keras_applications>=1.0.6 in c:\\users\\david\\anaconda3\\setup_dk\\lib\\site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: keras_preprocessing>=1.0.5 in c:\\users\\david\\anaconda3\\setup_dk\\lib\\site-packages (from keras) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of the import statements\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "import os\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop\n",
    "import pprint\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# datagen = ImageDataGenerator(width_shift_range=0.1, horizontal_flip=True)\n",
    "# datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6c8xzloyoUD"
   },
   "source": [
    "# Task 2\n",
    "Try to build a Neural Network model, train on the features and report the accuracy.\n",
    "Report your observations on the time taken on GPU and TPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a CNN based model with 5 hidden layers and 100 hidden units each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "#this is an empty sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=0,  # randomly rotates images from 0 to 180\n",
    "    # randomly shift images horizontally so that they are a fraction of total width\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically so that they are a fraction of total height\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,  # randomly flips images horizontally\n",
    "    vertical_flip=False)  # randomly flips images vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "#Lets print the shapes\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Converting class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CNN ####\n",
    "#5 hidden layers with 100 hidden units in each layer\n",
    "def my_model():\n",
    "    \n",
    "    #Adding Layer 1\n",
    "    #Each Layer has a Batch Normalization, Conv2D, MaxPooling2d, and Dropout function.\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))     \n",
    "    # Batch Normalization normalizes the input layer by adjusting and scaling the activations\n",
    "    model.add(tf.keras.layers.Conv2D(100, (5, 5), padding='same', activation='elu'))  \n",
    "    #filter, kernel size, padding, activation\n",
    "    #Shows the number of hidden units, shows the dimensions of the 2D-Array\n",
    "    #This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.\n",
    "    #ELU -> Exponential Linear Unit, converges cost and zeros faster\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))   \n",
    "    #Determines reduces the size of the data, number of parameters,\n",
    "    #the amount of computation needed, and controls overfitting\n",
    "    #pool_size = how much you down scale down data\n",
    "    model.add(tf.keras.layers.Dropout(0.25))     \n",
    "    #Dropout helps regularize the data and prevents overfitting\n",
    "\n",
    "    #Adding Layer 2, same code as layer 1, except activation\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:])) \n",
    "    model.add(tf.keras.layers.Conv2D(100, (5, 5), padding='same', activation='relu'))\n",
    "    #RELU ->Rectified Linear Units, , not linear, very powreful\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    #Adding Layer 3, same code as layer 2\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
    "    model.add(tf.keras.layers.Conv2D(100, (5, 5), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    #Adding Layer 4, same code as layer 2\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
    "    model.add(tf.keras.layers.Conv2D(100, (5, 5), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    #Adding Layer 5, same code as layer 2\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
    "    model.add(tf.keras.layers.Conv2D(100, (5, 5), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    #Flattening layers and kissing them together\n",
    "    model.add(tf.keras.layers.Flatten())#Returns a 1-D array\n",
    "    model.add(tf.keras.layers.Dense(256))#Use 1 hidden layer of 256 neurons, \n",
    "    model.add(tf.keras.layers.Activation('elu'))#Same as Layer 1\n",
    "    model.add(tf.keras.layers.Dropout(0.5))#Dropout helps regularize the data and prevents overfitting\n",
    "    model.add(tf.keras.layers.Dense(10))#Use 1 hidden layer of 10 neurons\n",
    "    model.add(tf.keras.layers.Activation('softmax'))#\n",
    "    return model\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\david\\Anaconda3\\Setup_DK\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 32, 32, 3)         12        \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 100)       7600      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 100)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 100)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 100)       400       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 100)       250100    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 100)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 100)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 100)         400       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 100)         250100    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 100)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 100)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 100)         400       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 100)         250100    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 100)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 2, 100)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 2, 100)         400       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 100)         250100    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,038,038\n",
      "Trainable params: 1,037,232\n",
      "Non-trainable params: 806\n",
      "_________________________________________________________________\n",
      "1.4055571556091309\n"
     ]
    }
   ],
   "source": [
    "start = time.time() #start measuring time\n",
    "model = my_model()\n",
    "model.summary() #show summary of model\n",
    "end = time.time() #end measuring time\n",
    "print(end - start) #printing the time taken on GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "#train\n",
    "model.compile(loss='categorical_crossentropy',optimizer='RMSprop',metrics=['accuracy'])\n",
    "#compiling the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\david\\Anaconda3\\Setup_DK\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\david\\Anaconda3\\Setup_DK\\lib\\threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\david\\Anaconda3\\Setup_DK\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 748, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\Users\\david\\Anaconda3\\Setup_DK\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 727, in pool_fn\n",
      "    initargs=(seqs, None, get_worker_id_queue()))\n",
      "  File \"C:\\Users\\david\\Anaconda3\\Setup_DK\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"C:\\Users\\david\\Anaconda3\\Setup_DK\\lib\\multiprocessing\\pool.py\", line 176, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\Users\\david\\Anaconda3\\Setup_DK\\lib\\multiprocessing\\pool.py\", line 241, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"C:\\Users\\david\\Anaconda3\\Setup_DK\\lib\\multiprocessing\\process.py\", line 112, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\Users\\david\\Anaconda3\\Setup_DK\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\Users\\david\\Anaconda3\\Setup_DK\\lib\\multiprocessing\\popen_spawn_win32.py\", line 89, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\Users\\david\\Anaconda3\\Setup_DK\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "TypeError: can't pickle _thread.lock objects\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time() #start measuring time\n",
    "# Computing the quantities required for feature-wise normalization\n",
    "datagen.fit(x_train)\n",
    "\n",
    "#Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(datagen.flow(x_train, y_train, \n",
    "                    batch_size=batch_size), \n",
    "                    epochs=epochs, \n",
    "                    validation_data=(x_test, y_test), \n",
    "                    workers=4,\n",
    "                    use_multiprocessing=True)\n",
    "end = time.time() #end measuring time\n",
    "print(end - start) #printing the time taken on GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN - Time taken on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 536.1103231906891 seconds to run on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN - Time taken on TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9797.70888876915 seconds to run on TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create an LSTM based model with 2 hidden layers and 1024 hidden units in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data augmentation\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Flatten X and reshape to prepare for model \n",
    "X_train_flattened = np.asarray(x_train.flatten()).reshape(50000,1,3072)\n",
    "X_test_flattened = np.asarray(x_test.flatten()).reshape(10000,1,3072)\n",
    "\n",
    "# Storing empty model in variable lstm_model\n",
    "lstm_model = keras.models.Sequential()\n",
    "\n",
    "# Add first LSTM layer with 1024 hidden units\n",
    "lstm_model.add(keras.layers.LSTM(1024, input_shape=(1, 3072), dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "\n",
    "# Add second LSTM layer with 1024 hidden units\n",
    "lstm_model.add(keras.layers.LSTM(1024, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# Create sigmoid activation layer\n",
    "lstm_model.add(keras.layers.Dense(10, activation='sigmoid'))\n",
    "\n",
    "# Try different optimizers and optimizer configs\n",
    "lstm_model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Display the summary of the model \n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.layers import Dense, Embedding\n",
    "# from keras.layers import LSTM\n",
    "# x_train_flattened = np.asarray(x_train.flatten()).reshape(50000,1,3072)\n",
    "# x_test_flatttened = np.asarray(x_test.flatten()).reshape(10000,1,3072)\n",
    "# lstm_model = keras.models.Sequential()\n",
    "# # Add first LSTM layer with 1024 hidden units\n",
    "# lstm_model.add(keras.layers.LSTM(1024, input_shape=(1, 3072), dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "# # Add second LSTM layer with 1024 hidden units\n",
    "# lstm_model.add(keras.layers.LSTM(1024, dropout=0.2, recurrent_dropout=0.2))\n",
    "# # Create sigmoid activation layer\n",
    "# lstm_model.add(keras.layers.Dense(10, activation='sigmoid'))\n",
    "# # Not sure if we need a softmax layer?\n",
    "# # Try different optimizers and optimizer configs\n",
    "# lstm_model.compile(loss='binary_crossentropy',\n",
    "#              optimizer='adam',\n",
    "#              metrics=['accuracy'])\n",
    "# lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's fit our data into the model and measure the time\n",
    "\n",
    "# Takes current time and stores it in start_time\n",
    "start_time = time.time()\n",
    "\n",
    "# This fits data into the model we created previously \n",
    "lstm_model.fit(X_train_flattened, y_train,\n",
    "         batch_size=32,\n",
    "         epochs=10,\n",
    "         validation_data=(X_test_flattened, y_test))\n",
    "\n",
    "# Measure ending time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print time it took to run\n",
    "print(end_time - start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_model.fit(x_train_flattened, y_train,\n",
    "#          batch_size=10,\n",
    "#          epochs=1,\n",
    "#          validation_data=(x_test_flatttened, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() #start measuring time\n",
    "model = my_model()\n",
    "model.summary() #show summary of model\n",
    "end = time.time() #end measuring time\n",
    "print(end - start) #printing the time taken on GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM - Time Taken on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "932.7716548442841 seconds in GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM - Time Taken on TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12634.27000324352 seconds in TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsE9N2WefgnI"
   },
   "source": [
    "# Task 3 (Extra credit, 25 points)\n",
    "Run the above on a TPU and report the time taken to fit the models. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
